---
title: "Generating CHI Prevalence Estimates"
format: gfm
prefer-html: false
self-contained: true
editor: visual
---

# Introduction

The `apde.chi.tools` package provides tools for a standardized workflow for preparing King County Community Health Indicators (CHI) estimates. This vignette demonstrates how to use the package's core functions to generate prevalence/proportion estimates from start to finish.

This workflow is specifically designed for calculating **prevalences and proportions**. If you need to calculate rates that use population denominators, please refer to the separate vignette for that purpose.

We'll walk through a complete analysis pipeline using birth data as an example, but the same workflow applies to other standardized datasets like ACS PUMS, BRFSS, and HYS.

The CHI standards are documented in [SharePoint \> Community Health Indicators \> CHI-Vizes](https://kc1.sharepoint.com/teams/DPH-CommunityHealthIndicators/CHIVizes/Forms/AllItems.aspx) \> CHI-Standards-TableauReady Output.xlsx. We'll follow these standards throughout our analysis.

Finally, remember that you can always get more information about a specific function by accessing its help file, e.g., `?chi_calc`, `?chi_generate_tro_shell`, etc.

# Load Packages

```{r setup}
#| warning: false
#| message: false

library(glue)           # For creating dynamic strings
library(future)         # For parallel processing
library(Microsoft365R)  # For SharePoint connections
library(DBI)            # For SQL Server connections
library(openxlsx)       # For Excel output
library(rads)           # For APDE analyses
library(data.table)     # For fast data manipulation
library(apde.chi.tools) # The package we're demonstrating
```

```{r pretty_kable}
#| warning: false
#| message: false
#| echo: false

# function to beautify tables
pretty_kable <- function(dt) {
  knitr::kable(dt, format = 'markdown')
}
```

# Analysis Configuration

First, let's set up our configuration parameters. This step defines key variables and paths used throughout the analysis. Doing this once at the top of your code will help you maintain and adapt it for subsequent years.

```{r config}

# Specify the most recent year available in the raw data
latest_year <- 2023

# Specify a directory for saving the output in the CHI SharePoint site
sharepoint_output_dir <- paste0('JUNK_testing/', latest_year, '-update/')
```

# Getting the Raw Data

Next, let's retrieve the birth data we'll be analyzing. The `rads::get_data_birth()` function pulls data from SQL, filtered to our specifications.

```{r get-data}
#| warning: false
#| message: false

# Get birth data for the past 10 years
birthsdt <- get_data_birth(
  cols = c("bigcities", "chi_geo_kc", "chi_geo_region", "chi_race_aic_asianother",
           "chi_race_aic_chinese", "chi_race_aic_filipino",
           "chi_race_aic_guam_or_chamorro", "chi_race_aic_hawaiian",
           "chi_race_aic_his_cuban", "chi_race_aic_his_mexican",
           "chi_race_aic_his_puerto_rican", "chi_race_aic_indian",
           "chi_race_aic_japanese", "chi_race_aic_korean", "chi_race_aic_samoan",
           "chi_race_aic_vietnamese", "edu_grp", "hra20_name", "mage5",
           "pov200grp", "race3", "race3_hispanic", "race4",
           "breastfed", 'bw_norm', 'chi_year', 'creation_date'),
  year = (latest_year-9):latest_year, # latest_year was defined above
  kingco = F)
```

# Open a Database Connection

Now, we need to connect to our SQL production database to access CHI estimates for the previous year.

```{r db-connection}
#| warning: false
#| message: false

db_chi_prod <- odbc::dbConnect(
  odbc::odbc(),
  Driver = "SQL Server",
  Server = "KCITSQLPRPHIP40", # the production server
  Database = "PHExtractStore")
```

# Getting the Analysis Set

Each CHI data source has an analysis set - a compact summary of all calculations needed for the indicators. It is often saved in the appropriate [GitHub repo](https://github.com/PHSKC-APDE/chi) sub-directory, along with your annual CHI code. However, if the analysis set is missing, the `chi_generate_analysis_set()` function can create it based on the latest year's results in the production server.

```{r analysis-set}
#| warning: false
#| message: false

# Generate the analysis set for birth data
analysis_sets <- chi_generate_analysis_set('birth')
```

Curious what an analysis set looks like? Let's take a peek at the first two rows:

```{r}
#| echo: false
# Show the last few rows to display structure
pretty_kable(head(analysis_sets[1:2]))
```

The analysis set contains important information about which:

-   indicators should be analyzed together (the `set` and `set_indicator_keys` columns)
-   category variables to use in analyses (`cat1`, `cat1_varname`)
-   analysis types to perform (`_kingcounty`, `_wastate`, `demgroups`, etc.)

# Generating Instructions

To analyze our data consistently, we need to generate a structured set of calculation instructions. The `chi_generate_tro_shell()` function creates these instructions based on our analysis set.

```{r generate-instructions}
#| warning: false
#| message: false

myinstructions <- chi_generate_tro_shell(
  ph.analysis_set = analysis_sets,
  end.year = latest_year,     # latest year to be used for aggregate estimates
  year.span = 5,              # number of years included in a single period
  trend.span = 3,             # number of years included in a trend single period
  trend.periods = 10          # number of periods to be included in a trend
)
```

Let's examine the structure of these instructions:

```{r}
#| echo: false
pretty_kable(head(myinstructions))
```

For this example, we'll focus on a single indicator to keep things manageable. Specifically, we've chosen the proportion of births with a normal birth weight, i.e., \[2,500g, 3,999g\].

```{r filter-instructions}
#| warning: false
#| message: false

myinstructions <- myinstructions[indicator_key %in% c('bw_norm')]
```

Now, let's clean up our instructions to prevent illogical geographic cross-tabulations. Note that this is one place where the analyst will need to think carefully and deeply. You should assume that this step will be specific to each analysis.

```{r tidy-instructions}
#| warning: false
#| message: false

# Prevent crosstabs of big cities with HRAs or Regions
myinstructions <- myinstructions[!(cat1_varname == 'bigcities' &
                                     cat2_varname %in% c('hra20_name', 'chi_geo_region'))]

# Prevent crosstabs of HRAs or Regions with big cities
myinstructions <- myinstructions[!(cat2_varname == 'bigcities' &
                                     cat1_varname %in% c('hra20_name', 'chi_geo_region'))]

```

# Performing Calculations

Now we're ready to perform our calculations using `chi_calc()`. This function applies our instructions to the raw data that we pulled from SQL, generating prevalence estimates with appropriate suppression and standard CHI columns.

## Configure Parallel Processing

The `chi_calc()` function utilizes parallel processing to speed up calculations. The following code optimizes parallel processing based on your machine’s resources. As of February 2025, APDE's performance laptops offer more available cores than our virtual machines (VMs). Therefore, `chi_calc()` is generally faster on a performance laptop compared to a VM.

```{r future-configuration}
#| warning: false
#| message: false

# Configures parallel processing using multiple sessions, reserving one core
future::plan(future::multisession, workers = future::availableCores() - 1)

# Sets the maximum memory (in GB) allowed per future process
future.GB = 2 
options(future.globals.maxSize = future.GB * 1024^3)
```

## Use `chi_calc()`

```{r calculate}
#| warning: false
#| message: false

myestimates <- chi_calc(ph.data = birthsdt,
                        ph.instructions = myinstructions,
                        ci = 0.90, 
                        rate = FALSE,
                        small_num_suppress = TRUE,
                        suppress_low = 0,
                        suppress_high = 9,
                        source_name = 'birth',
                        source_date = unique(birthsdt$creation_date))
```

Here are two rows from `myestimates` that will show the structure and contents of the table:

```{r calculate-display}
#| echo: false
# Show the last few rows to display structure
pretty_kable(head(myestimates[sample(1:.N, 2)]))
```

## Restoring Sequential Processing

After completing the parallel processing job, it's important to restore the system returns to its normal state.

```{r future-revert}
future::plan(future::sequential)
```

# Tidying `chi_calc()` output

## Drop the `level` column

The `chi_calc()` function returns a column called `level`, which contains the specific factor level for categorical variables. For example, if `indicator_key == 'fetal_pres'`, the function would return separate rows for each possible presentation: a row with `level == 'Breech'`, another with `level == 'Cephalic'`, and another with `level == 'Other'`. In these cases, we would use the `level` column to filter for our factor level of interest.

In our current example, since `bw_norm` is not a categorical variable, the `level` column contains only `NA` values and can be safely dropped.

```{r tidy-level}
myestimates[, level := NULL]
```

## Race/Ethnicity

Due to the way that APDE decided to display race3 (Hispanic as ethnicity) and race4 (Hispanic as race), the calculation and preparation of these variables can be complex. We need to manipulate the results to align them with CHI standards.

For future reference, all CHI standards can be found in `rads.data::misc_chi_byvars` and [SharePoint \> Community Health Indicators \> CHI-Vizes](https://kc1.sharepoint.com/teams/DPH-CommunityHealthIndicators/CHIVizes/Forms/AllItems.aspx) \> CHI-Standards-TableauReady Output.xlsx.

```{r tidy-race}
#| warning: false
#| message: false

# For race4 categories (both cat1 and cat2)
myestimates[tab %in% c('demgroups', 'crosstabs') & cat1_varname == 'race4', 
            cat1 := "Birthing person's race"]
myestimates[tab %in% c('demgroups', 'crosstabs') & cat2_varname == 'race4', 
            cat2 := "Birthing person's race"]

# For race3 categories - default to race, override for Hispanic ethnicity
myestimates[tab %in% c('demgroups', 'crosstabs') & cat1_varname == 'race3', 
            cat1 := "Birthing person's race"]
myestimates[tab %in% c('demgroups', 'crosstabs') & cat1_varname == 'race3' & cat1_group == 'Hispanic', 
            cat1 := "Birthing person's ethnicity"]

myestimates[tab %in% c('demgroups', 'crosstabs') & cat2_varname == 'race3', 
            cat2 := "Birthing person's race"]
myestimates[tab %in% c('demgroups', 'crosstabs') & cat2_varname == 'race3' & cat2_group == 'Hispanic', 
            cat2 := "Birthing person's ethnicity"]

# Update trend data labels (both race3 and race4 at once)
myestimates[tab == 'trends' & cat1_varname %in% c('race3', 'race4'), 
            cat1 := "Birthing person's race/ethnicity"]
myestimates[tab == 'trends' & cat2_varname %in% c('race3', 'race4'), 
            cat2 := "Birthing person's race/ethnicity"]
```

# Updating Metadata

Next, we need to generate metadata for our calculated estimates. This step combines existing metadata with our current estimates calculated above.

```{r metadata}
#| warning: false
#| message: false

# Retrieve existing metadata from the database
metadata_old <- setDT(odbc::dbGetQuery(
  conn = db_chi_prod,
  statement = glue::glue_sql("SELECT * FROM [PHExtractStore].[APDE].[birth_metadata]
      WHERE indicator_key IN ({unique(myestimates$indicator_key)*})", .con = db_chi_prod)))

# Generate updated metadata
mymetadata <- chi_generate_metadata(meta.old = metadata_old,
                                    est.current = myestimates)
```

This is what the metadata table looks like:

```{r metadata-display}
#| echo: false
# Show the last few rows to display structure
pretty_kable(head(mymetadata[]))
```

# Quality Assurance

After calculation, we need to perform quality assurance checks to ensure our estimates and metadata conform to CHI standards. `chi_qa_tro` checks whether the CHI estimates and metadata are properly formatted, complete, and compliant with required standards, ensuring that column names, values, and data types meet specified criteria, such as proper rounding, absence of missing critical data, and consistency with reference tables. It also checks for issues like infinite values and ensures that specific data rules (e.g., proportions, bounds) are followed.

```{r qa}
#| message: false
#| output: false

# Perform QA checks
qa_result <- chi_qa_tro(CHIestimates = myestimates,
                        CHImetadata = mymetadata,
                        acs = F,
                        verbose = F)
```

The function returns 1 for pass, 0 for to warn of deviations from CHI standards

```{r qa-result}
#| message: true
if(qa_result == 1) {
  message("QA checks passed successfully!")
} else {
  message("QA checks identified at least one deviation from CHI standars. Run again with verbose = TRUE.")
}
```

# Comparing with Previous Estimates

An important validation step is comparing our new estimates with previous ones to identify 'notable differences'. The notable differences criteria were specified by Joie McCracken and are the same for each data source. They are used both for human QA and for sharing high level summaries to accompany new releases of CHI estimates.

```{r compare}
#| warning: false 
#| message: false

# Get previous _kingcounty and demgroups estimates from the database
estimates_old <- setDT(DBI::dbGetQuery(
  conn = db_chi_prod,
  statement = glue::glue_sql("SELECT * FROM [PHExtractStore].[APDE].[birth_results]
        WHERE tab IN ('_kingcounty', 'demgroups') AND chi = 1 AND
        indicator_key IN ({unique(myestimates$indicator_key)*})", .con = db_chi_prod)))

# Compare old and new estimates
mycomparison <- chi_compare_estimates(OLD = estimates_old,
                                      NEW = myestimates,
                                      OLD.year = paste0(latest_year-5, '-', latest_year-1),
                                      NEW.year = paste0(latest_year-4, '-', latest_year),
                                      META = mymetadata)
```

Let's examine the contents of `mycomparison`:

```{r}
#| echo: false
pretty_kable(mycomparison[1:3])
```

# Saving `mycomparison` to SharePoint

`mycomparison`, which contains absolute and relative differences and flags the notable differences, should be saved to SharePoint.

```{r save-mycomparison}
#| eval: true
#| echo: true
#| message: false
#| warning: false
#| output: false
#| results: "hide"

# Connect to SharePoint
team <- get_team("Community Health Indicators")
drv <- team$get_drive("CHI-Vizes")

# Create a temporary file to store mycomparison as an Excel file
tempy <- tempfile(fileext = ".xlsx")

# Write mycomparison to the temporary Excel file
openxlsx::write.xlsx(x = mycomparison,
                     file = tempy,
                     asTable = TRUE,    # Ensure data is written as a table
                     overwrite = TRUE,  # Allow overwriting the file if it exists
                     tableStyle = "TableStyleMedium9")  # Apply a predefined style

# Upload the Excel file to SharePoint
drv$upload_file(src = tempy,
                dest = paste0(sharepoint_output_dir,
                             "qa_result_old_vs_new_",
                             latest_year-4, "_", latest_year, ".xlsx"))

rm(tempy)
```

# Saving Estimates & Metadata SQL Server

We need to save our results and metadata to the development SQL Server. Later, once it passes human QA, it will be transferred to the production SQL Server.

```{r save-sql}
#| warning: false

chi_update_sql(CHIestimates = myestimates,
               CHImetadata = mymetadata,
               table_name = 'junk', # replace with data source name, e.g., 'birth', 'brfss', etc.
               server = 'development', 
               replace_table = FALSE)
```

# Exporting Estimates & Metadata to Excel

Finally, save a copy of our estimates and metadata to an Excel file in SharePoint:

```{r save-excel}
#| eval: true
#| echo: true
#| message: false
#| warning: false
#| output: false
#| results: "hide"

# Connect to SharePoint
team <- get_team("Community Health Indicators")
drv <- team$get_drive("CHI-Vizes")

# Create Excel workbook with estimates and metadata
tempy <- tempfile(fileext = ".xlsx")
wb <- openxlsx::createWorkbook()

# Add worksheets and write data
openxlsx::addWorksheet(wb, "Estimates")
openxlsx::writeDataTable(wb, "Estimates",
                         x = myestimates,
                         tableStyle = "TableStyleMedium9")

openxlsx::addWorksheet(wb, "Metadata")
openxlsx::writeDataTable(wb, "Metadata",
                         x = mymetadata,
                         tableStyle = "TableStyleMedium9")

# Save workbook to tempfile
openxlsx::saveWorkbook(wb, file = tempy, overwrite = TRUE)

# Upload to SharePoint
drv$upload_file(src = tempy,
                dest = paste0(sharepoint_output_dir,
                             "Tableau_Ready_",
                             latest_year-4, "_", latest_year, ".xlsx"))
rm(tempy)
```

# Delete Temporary Tables

In the process of creating this vignette, we created some temporary tables on SharePoint and SQL Server. Let's delete these tables.

```{r clean-up-temp-SQL}
#| eval: true
#| echo: true
#| message: false
#| warning: false
#| output: false
#| results: "hide"

# Drop SharePoint directory
SharePoint_Parent <- strsplit(sharepoint_output_dir, "/")[[1]][1]
fff = drv$get_item(SharePoint_Parent)$delete(confirm = FALSE)

# Drop the SQL Server tables
db_chi_dev <- odbc::dbConnect(
  odbc::odbc(),
  Driver = "SQL Server",
  Server = "KCITSQLUATHIP40", # dev server
  Database = "PHExtractStore")

DBI::dbExecute(conn = db_chi_dev, "DROP TABLE [PHExtractStore].[APDE_WIP].[junk_results]")
DBI::dbExecute(conn = db_chi_dev, "DROP TABLE [PHExtractStore].[APDE_WIP].[junk_metadata]")

```

# Conclusion

Congratulations on completing the CHI analysis workflow! This workflow provides a standardized process for generating prevalence estimates, ensuring consistency and traceability. Follow these steps to streamline your analysis and maintain CHI standards across datasets.

New functions you used:

-   `chi_generate_analysis_set()` to create the analysis set
-   `chi_generate_tro_shell()` to generate calculation instructions
-   `chi_calc()` to perform the actual calculations
-   `chi_generate_metadata()` to create an updated metadata table
-   `chi_qa_tro()` to perform quality assurance checks
-   `chi_compare_estimates()` to identify notable differences compared to previous estimates
-   `chi_update_sql()` to save estimates and metadata to SQL Server

If you encounter issues or believe you've found a bug, please submit a GitHub issue at [Issues · PHSKC-APDE/apde.chi.tools](https://github.com/PHSKC-APDE/apde.chi.tools/issues).

Remember that this workflow is specifically for prevalence/proportion calculations. For rate calculations that use population denominators, please refer to the dedicated rate calculation vignette.

-- *Updated `r Sys.Date()`*
